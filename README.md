# tabnine + swe bench eval
This evaluation is forked from https://github.com/swe-bench/SWE-bench, in attempt to provide a framework for us to test in the lab the effect of different retrieval and/or llms capabilities on code generation tasks. 

## how to run the evaluation
first, create 